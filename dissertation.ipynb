{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7872f720-e4a2-479c-bf0f-37578a97f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "log_return_path = r'C:\\Users\\ahmed\\Downloads\\ref_log_return (1).pkl'\n",
    "price_path = r'C:\\Users\\ahmed\\Downloads\\ref_price (1).pkl'\n",
    "\n",
    "with open(log_return_path, 'rb') as f:\n",
    "    log_returns = pickle.load(f)\n",
    "\n",
    "with open(price_path, 'rb') as f:\n",
    "    prices = pickle.load(f)\n",
    "\n",
    "log_returns = np.array(log_returns)  \n",
    "prices = np.array(prices)          \n",
    "log_returns_reshaped = log_returns.reshape(-1, 3)\n",
    "log_return_df = pd.DataFrame(log_returns_reshaped, columns=['Crypto1_LogRet', 'Crypto2_LogRet', 'Crypto3_LogRet'])\n",
    "\n",
    "prices_reshaped = prices.reshape(-1, 3)\n",
    "price_df = pd.DataFrame(prices_reshaped, columns=['Crypto1_Price', 'Crypto2_Price', 'Crypto3_Price'])\n",
    "\n",
    "log_return_info = {\n",
    "    \"Shape\": log_returns.shape,\n",
    "    \"Null Values\": np.isnan(log_returns).sum(),\n",
    "    \"Per-Crypto Mean\": log_return_df.mean().to_dict(),\n",
    "    \"Per-Crypto Std Dev\": log_return_df.std().to_dict(),\n",
    "    \"Min/Max\": log_return_df.describe().loc[['min', 'max']].to_dict()\n",
    "}\n",
    "\n",
    "price_info = {\n",
    "    \"Shape\": prices.shape,\n",
    "    \"Null Values\": np.isnan(prices).sum(),\n",
    "    \"Per-Crypto Mean\": price_df.mean().to_dict(),\n",
    "    \"Per-Crypto Std Dev\": price_df.std().to_dict(),\n",
    "    \"Min/Max\": price_df.describe().loc[['min', 'max']].to_dict()\n",
    "}\n",
    "\n",
    "(log_return_info, price_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e73a9e-22db-4f30-a9e3-137a34c0be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_return_path = 'C:/Users/ahmed/Downloads/ref_log_return (1).pkl'\n",
    "price_path = 'C:/Users/ahmed/Downloads/ref_price (1).pkl'\n",
    "\n",
    "with open(log_return_path, 'rb') as f:\n",
    "    log_returns = pickle.load(f)\n",
    "\n",
    "with open(price_path, 'rb') as f:\n",
    "    prices = pickle.load(f)\n",
    "\n",
    "log_returns = np.array(log_returns)  \n",
    "prices = np.array(prices)           \n",
    "\n",
    "log_return_shape = log_returns.shape\n",
    "price_shape = prices.shape\n",
    "\n",
    "log_return_mean = np.mean(log_returns, axis=(0, 1))\n",
    "log_return_std = np.std(log_returns, axis=(0, 1))\n",
    "\n",
    "price_mean = np.mean(prices, axis=(0, 1))\n",
    "price_std = np.std(prices, axis=(0, 1))\n",
    "\n",
    "sample_indices = np.random.choice(log_returns.shape[0], size=3, replace=False)\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    for j in range(3):\n",
    "        plt.plot(log_returns[sample_indices[i], :, j], label=f'Crypto {j+1}')\n",
    "    plt.title(f'Log-Return Series Sample {i+1}')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Log Return')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i+1} initial prices: {prices[sample_indices[i], 0]}\")\n",
    "\n",
    "(log_return_shape, price_shape, log_return_mean, log_return_std, price_mean, price_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f2f45-21aa-4dd9-997e-8985d6e28e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "log_return_path = r'C:\\Users\\ahmed\\Downloads\\ref_log_return (1).pkl'\n",
    "price_path = r'C:\\Users\\ahmed\\Downloads\\ref_price (1).pkl'\n",
    "\n",
    "with open(log_return_path, 'rb') as f:\n",
    "    log_returns = pickle.load(f)\n",
    "\n",
    "with open(price_path, 'rb') as f:\n",
    "    prices = pickle.load(f)\n",
    "\n",
    "log_returns = np.array(log_returns) \n",
    "prices = np.array(prices)          \n",
    "\n",
    "sample_day_index = 0\n",
    "log_return_df = pd.DataFrame(log_returns[sample_day_index], columns=['Crypto1_LogRet', 'Crypto2_LogRet', 'Crypto3_LogRet'])\n",
    "print(f\"Log Returns for Day {sample_day_index}:\\n\")\n",
    "print(log_return_df)\n",
    "\n",
    "price_row = prices[sample_day_index][0]  # Shape (3,)\n",
    "price_df = pd.DataFrame([price_row], columns=['Crypto1_Price', 'Crypto2_Price', 'Crypto3_Price'])\n",
    "print(f\"\\nInitial Prices for Day {sample_day_index}:\\n\")\n",
    "print(price_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa29658-86b7-4ec4-8dbc-b3330852458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Clamp extreme outliers to [-0.1, 0.1]\n",
    "log_returns_clamped = np.clip(log_returns, -0.1, 0.1)\n",
    "\n",
    "# 2. Standardize (z-score) across all days and cryptos\n",
    "mean = log_returns_clamped.mean(axis=(0, 1), keepdims=True)\n",
    "std = log_returns_clamped.std(axis=(0, 1), keepdims=True)\n",
    "log_returns_norm = (log_returns_clamped - mean) / std\n",
    "\n",
    "# Save the mean and std for use during generation/evaluation\n",
    "np.save(\"log_return_mean.npy\", mean)\n",
    "np.save(\"log_return_std.npy\", std)\n",
    "\n",
    "# 3. Split into training and validation sets\n",
    "train_data, val_data = train_test_split(log_returns_norm, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Train shape:\", train_data.shape)  \n",
    "print(\"Val shape:\", val_data.shape)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ee43bc-d916-4b5d-a8cc-6523422c2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 5 random samples of log-returns for Crypto1\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):\n",
    "    sample_idx = np.random.randint(0, train_data.shape[0])\n",
    "    plt.plot(train_data[sample_idx, :, 0], label=f'Sample {i+1}')\n",
    "plt.title('Sample Log-Return Paths for Crypto1 (Normalized)')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Log Return (Standardized)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda663b-d41a-4525-a859-94e98697be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(train_data.flatten(), bins=100, color='skyblue', edgecolor='k')\n",
    "plt.title(\"Histogram of Normalized Log-Returns (All Cryptos)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d9045-6829-4583-b975-2240884ea666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Reshape for boxplot\n",
    "reshaped = train_data.reshape(-1, 3)\n",
    "df_box = pd.DataFrame(reshaped, columns=[\"Crypto1\", \"Crypto2\", \"Crypto3\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df_box)\n",
    "plt.title(\"Boxplot of Normalized Log-Returns per Crypto\")\n",
    "plt.ylabel(\"Normalized Log-Return\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7e067-cb64-42ec-85bf-02c49697ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_box.corr()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Cryptos (Normalized Returns)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f26b92-13de-47d6-b70c-2c9c35850801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Use your training data (shape: [days, hours, cryptos])\n",
    "train_data = train_data  # already normalized log-returns\n",
    "val_data = val_data      # for comparison\n",
    "\n",
    "# Flatten across days & hours for each crypto\n",
    "crypto_series = {\n",
    "    f\"Crypto{i+1}\": train_data[:,:,i].reshape(-1) for i in range(train_data.shape[2])\n",
    "}\n",
    "\n",
    "def fit_best_arima(series, p_max=3, q_max=3):\n",
    "    best = None\n",
    "    for p in range(p_max+1):\n",
    "        for q in range(q_max+1):\n",
    "            try:\n",
    "                model = ARIMA(series, order=(p,0,q))\n",
    "                res = model.fit()\n",
    "                if best is None or res.aic < best[0]:\n",
    "                    best = (res.aic, res)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return best[1]\n",
    "\n",
    "def simulate_arima(res, T, M=500, burn=50):\n",
    "    \"\"\"Simulate M paths length T from fitted ARIMA\"\"\"\n",
    "    sims = []\n",
    "    for _ in range(M):\n",
    "        path = res.simulate(T+burn)\n",
    "        sims.append(np.asarray(path)[burn:])  # discard burn-in\n",
    "    return np.vstack(sims)  # shape [M, T]\n",
    "\n",
    "def var_es(x, alpha):\n",
    "    q = np.quantile(x, alpha)\n",
    "    es = x[x <= q].mean() if (x <= q).any() else q\n",
    "    return q, es\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Evaluate each crypto\n",
    "for name, series in crypto_series.items():\n",
    "    # Fit ARIMA\n",
    "    res = fit_best_arima(series)\n",
    "\n",
    "    # Simulate synthetic returns (match validation length)\n",
    "    T = val_data.shape[1]  # hours per day\n",
    "    M = val_data.shape[0]  # number of days\n",
    "    synth = simulate_arima(res, T, M=M)  # shape [M, T]\n",
    "    flat = synth.ravel()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"Skew\": skew(flat),\n",
    "        \"Kurtosis\": kurtosis(flat, fisher=True)  # excess kurtosis\n",
    "    }\n",
    "    for a in [0.01, 0.05, 0.10]:\n",
    "        q, es = var_es(flat, a)\n",
    "        metrics[f\"VaR@{a}\"] = q\n",
    "        metrics[f\"ES@{a}\"] = es\n",
    "    results[name] = metrics\n",
    "\n",
    "# Make table\n",
    "arima_df = pd.DataFrame(results).T\n",
    "print(arima_df)\n",
    "\n",
    "# Save results\n",
    "arima_df.to_csv(\"arima_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c7179-3cb1-44f0-b3c4-186a51af8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "class CryptoLogReturnDataset(Dataset):\n",
    "    def __init__(self, data_array):\n",
    "        self.data = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CryptoLogReturnDataset(train_data)\n",
    "val_dataset = CryptoLogReturnDataset(val_data)\n",
    "\n",
    "real_data = train_data  \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Dimensionality parameters\n",
    "seq_len = train_data.shape[1]   \n",
    "feature_dim = train_data.shape[2]  \n",
    "input_dim = seq_len * feature_dim  \n",
    "noise_dim = 128  \n",
    "\n",
    "\n",
    "lambda_tail = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86b35d-4774-49a9-987f-2f7d67dfcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === Generator ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, output_dim=72):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.model(z)\n",
    "        noise = 0.05 * torch.randn_like(out) \n",
    "        out = out + noise\n",
    "        return out.view(z.size(0), 24, 3\n",
    "\n",
    "# === Discriminator ===\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=72):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  \n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8ede2-1516-4af9-8edd-0b76ce1b874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 128\n",
    "output_dim = 72 \n",
    "\n",
    "# === Instantiate models ===\n",
    "generator = Generator(noise_dim, output_dim).to(device)\n",
    "discriminator = Discriminator(output_dim).to(device)\n",
    "\n",
    "# === Loss function ===\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === Optimizers ===\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ad3f9-6caa-4a32-9295-43e3c17e7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy.stats import skew as sp_skew, kurtosis as sp_kurt\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- models ---\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "# --- loss & opt (standard GAN) ---\n",
    "criterion   = nn.BCEWithLogitsLoss()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(),     lr=2e-4, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "#  EMA for smoother sampling/eval ---\n",
    "use_ema  = True\n",
    "ema_decay = 0.999\n",
    "if use_ema:\n",
    "    G_ema = type(generator) (*generator.__dict__['_modules'].values()) if False else None  # placeholder\n",
    "\n",
    "    import copy\n",
    "    G_ema = copy.deepcopy(generator).to(device)\n",
    "    for p in G_ema.parameters(): p.requires_grad_(False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(target, source, decay=0.999):\n",
    "    for p_t, p_s in zip(target.parameters(), source.parameters()):\n",
    "        p_t.data.mul_(decay).add_(p_s.data, alpha=1.0 - decay)\n",
    "\n",
    "# --- training ---\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    generator.train(); discriminator.train()\n",
    "    for real_batch in train_loader:\n",
    "        real_batch = real_batch.to(device)  # [B, T, A]\n",
    "        B = real_batch.size(0)\n",
    "\n",
    "\n",
    "        # 1) Train D\n",
    "\n",
    "        z = torch.randn(B, noise_dim, device=device)\n",
    "        with torch.no_grad():\n",
    "            fake_detach = generator(z)\n",
    "\n",
    "        real_labels = torch.empty(B, 1, device=device).uniform_(0.9, 1.0)  # label smoothing\n",
    "        fake_labels = torch.empty(B, 1, device=device).uniform_(0.0, 0.1)\n",
    "\n",
    "        d_optimizer.zero_grad(set_to_none=True)\n",
    "        d_real = discriminator(real_batch)\n",
    "        d_fake = discriminator(fake_detach)\n",
    "        d_loss = criterion(d_real, real_labels) + criterion(d_fake, fake_labels)\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "\n",
    "        # 2) Train G (adv only)\n",
    "\n",
    "        z = torch.randn(B, noise_dim, device=device)\n",
    "        fake_batch = generator(z)\n",
    "\n",
    "        g_optimizer.zero_grad(set_to_none=True)\n",
    "        d_fake = discriminator(fake_batch)\n",
    "        # try to fool D as \"real\"\n",
    "        g_loss = criterion(d_fake, real_labels)\n",
    "        g_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(generator.parameters(), max_norm=5.0)  # optional, harmless\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if use_ema:\n",
    "            ema_update(G_ema, generator, ema_decay)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | D: {d_loss.item():.4f} | G: {g_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2982b02-3c2a-4e5b-8864-d97a145e4d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55654b2d-f9e7-423e-97da-689e52b6a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(5000, noise_dim).to(device)\n",
    "    synthetic_batch = generator(noise).cpu().numpy()  # shape: (5000, 24, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37fca6-9c1a-4ce6-99cd-7bf073334320",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_returns = synthetic_batch.reshape(-1)\n",
    "real_returns = real_data.reshape(-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d64216-2595-411f-9b0c-3dc16540a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(real_returns, label='Real', bw_adjust=1.5)\n",
    "sns.kdeplot(synthetic_returns, label='Synthetic (Tail-GAN)', bw_adjust=1.5)\n",
    "plt.title(\"Log-Return Distribution: Real vs Tail-GAN\")\n",
    "plt.xlabel(\"Log Return\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34150f01-3e96-4c0d-9745-06a31dad2a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_synthetic(G, n_samples=5000, noise_dim=128, device=\"cpu\"):\n",
    "    G.eval()\n",
    "    z = torch.randn(n_samples, noise_dim, device=device)\n",
    "    synth = G(z).detach().cpu().numpy()  \n",
    "    return synth\n",
    "\n",
    "def evaluate_gan(real_data, synthetic_data, alpha=0.05, fisher_excess=True):\n",
    "\n",
    "    def to_flat(x):\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        elif x.ndim == 3:\n",
    "            x = x.reshape(-1, x.shape[2])\n",
    "        return x\n",
    "\n",
    "    real_flat  = to_flat(real_data)\n",
    "    synth_flat = to_flat(synthetic_data)\n",
    "    A = real_flat.shape[1]\n",
    "\n",
    "    print(f\"{'Metric':<12} | \" + \" | \".join([f\"Asset {i+1:>2}\" for i in range(A)]))\n",
    "    print(\"-\" * (14 + A*14))\n",
    "\n",
    "    # VaR\n",
    "    real_var  = [np.quantile(real_flat[:, i], alpha)  for i in range(A)]\n",
    "    synth_var = [np.quantile(synth_flat[:, i], alpha) for i in range(A)]\n",
    "    print(\"VaR (5%)    | \" + \" | \".join([f\"{v:10.5f}\" for v in real_var]))\n",
    "    print(\"            | \" + \" | \".join([f\"{v:10.5f}\" for v in synth_var]))\n",
    "\n",
    "    # ES\n",
    "    real_es  = [real_flat[:, i][real_flat[:, i]  <= real_var[i]].mean()  for i in range(A)]\n",
    "    synth_es = [synth_flat[:, i][synth_flat[:, i] <= synth_var[i]].mean() for i in range(A)]\n",
    "    print(\"ES (5%)     | \" + \" | \".join([f\"{e:10.5f}\" for e in real_es]))\n",
    "    print(\"            | \" + \" | \".join([f\"{e:10.5f}\" for e in synth_es]))\n",
    "\n",
    "    # Skew\n",
    "    real_sk = [sp_skew(real_flat[:, i],  bias=False) for i in range(A)]\n",
    "    syn_sk  = [sp_skew(synth_flat[:, i], bias=False) for i in range(A)]\n",
    "    print(\"Skewness    | \" + \" | \".join([f\"{s:10.5f}\" for s in real_sk]))\n",
    "    print(\"            | \" + \" | \".join([f\"{s:10.5f}\" for s in syn_sk]))\n",
    "\n",
    "    # Kurtosis \\\n",
    "    real_ku = [sp_kurt(real_flat[:, i],  bias=False, fisher=fisher_excess) for i in range(A)]\n",
    "    syn_ku  = [sp_kurt(synth_flat[:, i], bias=False, fisher=fisher_excess) for i in range(A)]\n",
    "    print(\"Kurtosis    | \" + \" | \".join([f\"{k:10.5f}\" for k in real_ku]))\n",
    "    print(\"            | \" + \" | \".join([f\"{k:10.5f}\" for k in syn_ku]))\n",
    "\n",
    "    return {\n",
    "        \"VaR_real\": np.array(real_var),   \"VaR_synth\": np.array(synth_var),\n",
    "        \"ES_real\":  np.array(real_es),    \"ES_synth\":  np.array(synth_es),\n",
    "        \"Skew_real\": np.array(real_sk),   \"Skew_synth\": np.array(syn_sk),\n",
    "        \"Kurt_real\": np.array(real_ku),   \"Kurt_synth\": np.array(syn_ku),\n",
    "    }\n",
    "\n",
    "\n",
    "G_eval = G_ema if use_ema else generator\n",
    "synthetic = sample_synthetic(G_eval, n_samples=5000, noise_dim=noise_dim, device=device)\n",
    "\n",
    "\n",
    "\n",
    "_ = evaluate_gan(real_data, synthetic, alpha=0.05, fisher_excess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85017d-617c-47b8-ae77-c3ea3d386f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.distributions import StudentT\n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "class CryptoLogReturnDataset(Dataset):\n",
    "    def __init__(self, data_array):\n",
    "        self.data = torch.tensor(data_array, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = CryptoLogReturnDataset(train_data)\n",
    "val_dataset   = CryptoLogReturnDataset(val_data)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64, shuffle=True,  drop_last=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=64, shuffle=False, drop_last=False)\n",
    "\n",
    "seq_len, feature_dim = train_data.shape[1], train_data.shape[2]   # e.g., 24, 3\n",
    "noise_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208815bd-7cc7-44e3-8a08-bddd449b5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.distributions import StudentT\n",
    "import os\n",
    "\n",
    "\n",
    "# 1) Small, stable TCN blocks\n",
    "\n",
    "class ResTCNBlock(nn.Module):\n",
    "    \"\"\"Residual 1D conv block with dilation; GroupNorm + GELU for small batches.\"\"\"\n",
    "    def __init__(self, channels: int, dilation: int = 1, dropout: float = 0.05):\n",
    "        super().__init__()\n",
    "        pad = dilation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, 3, padding=pad, dilation=dilation),\n",
    "            nn.GroupNorm(num_groups=min(8, channels), num_channels=channels),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv1d(channels, channels, 3, padding=pad, dilation=dilation),\n",
    "            nn.GroupNorm(num_groups=min(8, channels), num_channels=channels),\n",
    "        )\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(x + self.net(x))\n",
    "\n",
    "class TailGANGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, noise_dim=128, seq_len=24, feature_dim=3,\n",
    "                 hidden_channels=128, num_blocks=4, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden = hidden_channels\n",
    "\n",
    "        self.fc = nn.Linear(noise_dim, hidden_channels * seq_len)\n",
    "        dilations = [1, 2, 4, 8][:num_blocks]\n",
    "        self.blocks = nn.ModuleList([ResTCNBlock(hidden_channels, d, dropout) for d in dilations])\n",
    "        self.to_out = nn.Conv1d(hidden_channels, feature_dim, kernel_size=1)\n",
    "        self.out_scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, z):\n",
    "        B = z.size(0)\n",
    "        x = self.fc(z).view(B, self.hidden, self.seq_len)  \n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.to_out(x) * self.out_scale                 \n",
    "        return x.permute(0, 2, 1)                          \n",
    "\n",
    "class MinibatchStdDev(nn.Module):\n",
    "    def __init__(self, eps=1e-8): super().__init__(); self.eps = eps\n",
    "    def forward(self, x):  \n",
    "        B, C, T = x.shape\n",
    "        if B < 2: return x\n",
    "        m = x.mean(dim=0, keepdim=True)\n",
    "        std = torch.sqrt((x - m).pow(2).mean(dim=0) + self.eps) \n",
    "        ch  = std.mean().view(1,1,1).expand(B,1,T)\n",
    "        return torch.cat([x, ch], dim=1)  \n",
    "\n",
    "class TailGANDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len=24, feature_dim=3, base_channels=64, dropout=0.05):\n",
    "        super().__init__()\n",
    "        C = base_channels\n",
    "        self.conv_in  = spectral_norm(nn.Conv1d(feature_dim, C,   3, padding=1, stride=2))\n",
    "        self.conv_mid = spectral_norm(nn.Conv1d(C, C,             3, padding=1))\n",
    "        self.mbstd    = MinibatchStdDev()\n",
    "        self.conv_out = spectral_norm(nn.Conv1d(C+1, 2*C,         3, padding=1, stride=2))\n",
    "        self.act  = nn.LeakyReLU(0.2)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.gap  = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc   = spectral_norm(nn.Linear(2*C, 1))\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=0.2)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x.permute(0,2,1)                 \n",
    "        x = self.act(self.conv_in(x))        \n",
    "        x = self.drop(self.act(self.conv_mid(x)))\n",
    "        x = self.mbstd(x)                    \n",
    "        x = self.drop(self.act(self.conv_out(x)))  \n",
    "        x = self.gap(x).squeeze(-1)          \n",
    "        return self.fc(x)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98df604-a6a1-4c7f-b2f9-302451d68195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_shortfall(x, alpha=0.05, dim=0):\n",
    "    k = max(1, int(alpha * x.size(dim)))\n",
    "    vals, _ = torch.sort(x, dim=dim)\n",
    "    return vals.narrow(dim, 0, k).mean(dim=dim)\n",
    "\n",
    "def es_matching_loss_multi(real, fake, alphas=(0.01,0.05,0.10)):\n",
    "    # real/fake: [B,T,A] → compare ES per asset over [B*T]\n",
    "    B,T,A = real.shape\n",
    "    r = real.reshape(B*T, A); f = fake.reshape(B*T, A)\n",
    "    losses = []\n",
    "    for a in alphas:\n",
    "        es_r = expected_shortfall(r, a, dim=0)\n",
    "        es_f = expected_shortfall(f, a, dim=0)\n",
    "        losses.append((es_f - es_r).abs().mean())\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def quantile_pinball_loss(y_pred, y_true, q=0.05):\n",
    "    e = y_true - y_pred\n",
    "    return torch.mean(torch.maximum(q*e, (q-1)*e))\n",
    "\n",
    "def pinball_multi(fake, real,\n",
    "                  qs_lower=(0.01,0.05,0.10), w_lower=(0.6,0.3,0.1),\n",
    "                  qs_upper=(0.90,),         w_upper=(1.0,)):\n",
    "    # sample one timestep to align shapes\n",
    "    B,T,A = real.shape\n",
    "    idx_t = torch.randint(0, T, (B,), device=real.device)\n",
    "    r = real[torch.arange(B), idx_t, :]  # [B,A]\n",
    "    f = fake[torch.arange(B), idx_t, :]  # [B,A]\n",
    "    low = torch.stack([w*quantile_pinball_loss(f, r, q=q) for q,w in zip(qs_lower, w_lower)]).sum()\n",
    "    up  = torch.stack([w*quantile_pinball_loss(-f, -r, q=1-q) for q,w in zip(qs_upper, w_upper)]).sum()\n",
    "    return low, up\n",
    "\n",
    "def _centered_moments(x):\n",
    "    m = x.mean(dim=0); xc = x - m\n",
    "    v = (xc**2).mean(dim=0) + 1e-12\n",
    "    s = torch.sqrt(v)\n",
    "    m3 = (xc**3).mean(dim=0)\n",
    "    m4 = (xc**4).mean(dim=0)\n",
    "    skew = m3 / (s**3)\n",
    "    kurt_excess = m4 / (v**2) - 3.0\n",
    "    return skew, kurt_excess\n",
    "\n",
    "def _compress_kurt(k):\n",
    "    # compress very large excess-kurtosis to stabilise optimisation\n",
    "    return torch.sign(k) * torch.log1p(k.abs())\n",
    "\n",
    "def moment_matching_losses(real, fake):\n",
    "    \"\"\"Return separate skew and kurt losses (L1), robust kurt via log1p.\"\"\"\n",
    "    B,T,A = real.shape\n",
    "    r = real.reshape(B*T, A); f = fake.reshape(B*T, A)\n",
    "    sr, kr = _centered_moments(r)\n",
    "    sf, kf = _centered_moments(f)\n",
    "    loss_skew = (sf - sr).abs().mean()\n",
    "    loss_kurt = (_compress_kurt(kf) - _compress_kurt(kr)).abs().mean()\n",
    "    return loss_skew, loss_kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6af38b-3427-4c62-bce9-48223cff84af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = TailGANGenerator(noise_dim=noise_dim, seq_len=seq_len, feature_dim=feature_dim).to(device)\n",
    "D = TailGANDiscriminator(seq_len=seq_len, feature_dim=feature_dim).to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# TTUR\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0.5,0.999))\n",
    "\n",
    "# EMA\n",
    "use_ema = True\n",
    "ema_decay = 0.999\n",
    "G_ema = TailGANGenerator(noise_dim=noise_dim, seq_len=seq_len, feature_dim=feature_dim).to(device)\n",
    "G_ema.load_state_dict(G.state_dict())\n",
    "for p in G_ema.parameters(): p.requires_grad_(False)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(target, source, decay=ema_decay):\n",
    "    for p_t, p_s in zip(target.parameters(), source.parameters()):\n",
    "        p_t.data.mul_(decay).add_(p_s.data, alpha=1-decay)\n",
    "\n",
    "# Student-t latent (heavier tails)\n",
    "student_t = StudentT(df=3.0)\n",
    "def sample_noise(B, dim, device):\n",
    "    return student_t.sample((B, dim)).to(device)\n",
    "\n",
    "\n",
    "LAMBDA_ES = 12    \n",
    "W_PIN_L   = 0.75   \n",
    "W_PIN_U   = 0.10    \n",
    "W_SKEW    = 1.0   \n",
    "W_KURT    = 0.20    \n",
    "\n",
    "#  early stop\n",
    "ckpt_dir = \"checkpoints\"; os.makedirs(ckpt_dir, exist_ok=True)\n",
    "best_score, best_epoch, no_improve = float(\"inf\"), -1, 0\n",
    "patience, save_every = 15, 10\n",
    "\n",
    "EPOCHS = 30 \n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    G.train(); D.train()\n",
    "    for real_batch in train_loader:\n",
    "        real_batch = real_batch.to(device)  # [B,T,A]\n",
    "        B = real_batch.size(0)\n",
    "\n",
    "        # ---- D step ----\n",
    "        z = sample_noise(B, noise_dim, device)\n",
    "        with torch.no_grad():\n",
    "            fake_detach = G(z)\n",
    "\n",
    "        real_labels = torch.empty(B,1,device=device).uniform_(0.9,1.0)\n",
    "        fake_labels = torch.empty(B,1,device=device).uniform_(0.0,0.1)\n",
    "\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "        d_real = D(real_batch)\n",
    "        d_fake = D(fake_detach)\n",
    "        d_loss = criterion(d_real, real_labels) + criterion(d_fake, fake_labels)\n",
    "        d_loss.backward(); opt_D.step()\n",
    "\n",
    "        # ---- G step ----\n",
    "        z = sample_noise(B, noise_dim, device)\n",
    "        fake_batch = G(z)\n",
    "\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "        d_fake = D(fake_batch)\n",
    "        gan_loss = criterion(d_fake, real_labels)\n",
    "\n",
    "        es_loss = es_matching_loss_multi(real_batch, fake_batch, alphas=(0.01,0.05,0.10))\n",
    "        q_low, q_up = pinball_multi(fake_batch, real_batch,\n",
    "                                    qs_lower=(0.01,0.05,0.10), w_lower=(0.6,0.3,0.1),\n",
    "                                    qs_upper=(0.90,),         w_upper=(1.0,))\n",
    "        skew_loss, kurt_loss = moment_matching_losses(real_batch, fake_batch)\n",
    "\n",
    "        g_loss = (gan_loss\n",
    "                  + LAMBDA_ES * es_loss\n",
    "                  + W_PIN_L   * q_low\n",
    "                  + W_PIN_U   * q_up\n",
    "                  + W_SKEW    * skew_loss\n",
    "                  + W_KURT    * kurt_loss)\n",
    "\n",
    "        g_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(G.parameters(), max_norm=5.0)\n",
    "        opt_G.step()\n",
    "\n",
    "        if use_ema:\n",
    "            ema_update(G_ema, G)\n",
    "\n",
    "\n",
    "    current_score = es_loss.item() + 0.5*skew_loss.item() + 0.5*kurt_loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"D: {d_loss.item():.4f} | G: {g_loss.item():.4f} | \"\n",
    "          f\"ES: {es_loss.item():.4f} | Qlow: {q_low.item():.4f} | Qup: {q_up.item():.4f} | \"\n",
    "          f\"Sk: {skew_loss.item():.4f} | Ku: {kurt_loss.item():.4f}\")\n",
    "\n",
    "    if current_score < best_score - 1e-4:\n",
    "        best_score, best_epoch, no_improve = current_score, epoch+1, 0\n",
    "        torch.save((G_ema if use_ema else G).state_dict(), f\"{ckpt_dir}/G_best.pt\")\n",
    "        torch.save(D.state_dict(), f\"{ckpt_dir}/D_best.pt\")\n",
    "        print(f\"✅ New BEST @ epoch {best_epoch} | score={best_score:.4f}\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if (epoch + 1) % save_every == 0:\n",
    "        torch.save((G_ema if use_ema else G).state_dict(), f\"{ckpt_dir}/G_epoch{epoch+1}.pt\")\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f\"⏹ Early stopping at epoch {epoch+1}. Best was {best_epoch} (score={best_score:.4f}).\")\n",
    "        break\n",
    "\n",
    "print(f\"Done. Best epoch: {best_epoch} with score={best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1d44c-9ca5-46ad-8c6a-4189d1c8553c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b145e-a2d0-4643-a7ff-dee4f02c3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df9bdc-be58-4ccc-97d7-f1c13d1565c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b7751-ba72-4f4d-a7ef-b824b2505431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900bbcf-9fce-40c8-a88a-8212e5f3910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import skew as sp_skew, kurtosis as sp_kurt\n",
    "\n",
    "@torch.no_grad()\n",
    "def _sample_noise(n_samples, noise_dim, device, latent=\"student_t\", df=3.0):\n",
    "    if latent.lower() == \"student_t\":\n",
    "        from torch.distributions import StudentT\n",
    "        return StudentT(df=df).sample((n_samples, noise_dim)).to(device)\n",
    "    return torch.randn(n_samples, noise_dim, device=device)\n",
    "\n",
    "def _to_numpy_nta(x):\n",
    "\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    x = np.asarray(x)\n",
    "    assert x.ndim == 3, f\"Expected [N,T,A], got shape {x.shape}\"\n",
    "    return x\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_gan_tail_metrics(\n",
    "    generator,           \n",
    "    real_data,           \n",
    "    noise_dim=128,\n",
    "    n_samples=4096,\n",
    "    device=\"cpu\",\n",
    "    alpha=0.05,\n",
    "    latent=\"student_t\",  \n",
    "    df=3.0,\n",
    "    fisher_excess=True,  \n",
    "    asset_names=None,    \n",
    "    print_table=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Samples synthetic paths from `generator` and compares per-asset VaR/ES/Skew/Kurt\n",
    "    against `real_data`. Also prints coverage: P[synth <= VaR_real].\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "\n",
    "    # --- data to numpy ---\n",
    "    real_np = _to_numpy_nta(real_data)           \n",
    "    N, T, A = real_np.shape\n",
    "\n",
    "    # --- sample synthetic ---\n",
    "    z = _sample_noise(n_samples, noise_dim, device, latent=latent, df=df)\n",
    "    synth_np = generator(z).detach().cpu().numpy()   \n",
    "\n",
    "    real_flat  = real_np.reshape(-1, A)           \n",
    "    synth_flat = synth_np.reshape(-1, A)            \n",
    "\n",
    "    # --- compute stats ---\n",
    "    real_var  = np.array([np.quantile(real_flat[:, i],  alpha) for i in range(A)])\n",
    "    synth_var = np.array([np.quantile(synth_flat[:, i], alpha) for i in range(A)])\n",
    "\n",
    "    real_es = np.array([\n",
    "        real_flat[:, i][real_flat[:, i] <= real_var[i]].mean()  for i in range(A)\n",
    "    ])\n",
    "    synth_es = np.array([\n",
    "        synth_flat[:, i][synth_flat[:, i] <= synth_var[i]].mean() for i in range(A)\n",
    "    ])\n",
    "\n",
    "    real_sk = np.array([sp_skew(real_flat[:, i],  bias=False) for i in range(A)])\n",
    "    syn_sk  = np.array([sp_skew(synth_flat[:, i], bias=False) for i in range(A)])\n",
    "\n",
    "    real_ku = np.array([sp_kurt(real_flat[:, i],  bias=False, fisher=fisher_excess) for i in range(A)])\n",
    "    syn_ku  = np.array([sp_kurt(synth_flat[:, i], bias=False, fisher=fisher_excess) for i in range(A)])\n",
    "\n",
    "  \n",
    "    coverage = np.array([\n",
    "        (synth_flat[:, i] <= real_var[i]).mean() for i in range(A)\n",
    "    ])\n",
    "\n",
    "\n",
    "    if print_table:\n",
    "        cols = [f\"Asset {i+1:>2}\" for i in range(A)] if asset_names is None else [f\"{n}\" for n in asset_names]\n",
    "        print(f\"{'Metric':<12} | \" + \" | \".join([f\"{c:>12}\" for c in cols]))\n",
    "        print(\"-\" * (14 + A*15))\n",
    "\n",
    "        def row(label, arr): return f\"{label:<12} | \" + \" | \".join([f\"{v:12.5f}\" for v in arr])\n",
    "\n",
    "        print(row(\"VaR_real\",  real_var))\n",
    "        print(row(\"VaR_synth\", synth_var))\n",
    "        print(row(\"ES_real\",   real_es))\n",
    "        print(row(\"ES_synth\",  synth_es))\n",
    "        print(row(\"Skew_real\", real_sk))\n",
    "        print(row(\"Skew_synth\",syn_sk))\n",
    "        print(row(\"Kurt_real\", real_ku))\n",
    "        print(row(\"Kurt_synth\",syn_ku))\n",
    "        print(row(\"Coverage@\", np.full(A, alpha))) \n",
    "        print(row(\"Cov_synth\", coverage))\n",
    "\n",
    "    return {\n",
    "        \"VaR_real\":  real_var,   \"VaR_synth\":  synth_var,\n",
    "        \"ES_real\":   real_es,    \"ES_synth\":   synth_es,\n",
    "        \"Skew_real\": real_sk,    \"Skew_synth\": syn_sk,\n",
    "        \"Kurt_real\": real_ku,    \"Kurt_synth\": syn_ku,\n",
    "        \"Coverage_realVaR\": coverage,         \n",
    "        \"synthetic\": synth_np                  \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "metrics = evaluate_gan_tail_metrics(\n",
    "    generator=G_ema if 'G_ema' in globals() else generator,\n",
    "    real_data=real_data,\n",
    "    noise_dim=noise_dim,\n",
    "    n_samples=4096,\n",
    "    device=device,\n",
    "    alpha=0.05,\n",
    "    latent=\"student_t\",   \n",
    "    df=3.0,\n",
    "    fisher_excess=True,\n",
    "    asset_names=[f\"Asset {i+1}\" for i in range(real_data.shape[2])],\n",
    "    print_table=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f32947-c185-47ee-a7a1-d6ec62bda59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- auto-pick REAL & SYNTHETIC ----------\n",
    "def _as_np(x): return x if isinstance(x, np.ndarray) else x.detach().cpu().numpy()\n",
    "\n",
    "g = globals()\n",
    "\n",
    "for key in [\"real_data\", \"val_data\", \"train_data\", \"real_returns\"]:\n",
    "    if key in g:\n",
    "        real = _as_np(g[key]); break\n",
    "else:\n",
    "    raise NameError(\"Define one of real_data / val_data / train_data / real_returns (shape [N,T,A]).\")\n",
    "\n",
    "for key in [\"synthetic\", \"synthetic_returns\", \"synthetic_batch\", \"vanilla_synth\", \"cgan_synth\"]:\n",
    "    if key in g:\n",
    "        synthetic = _as_np(g[key]); break\n",
    "else:\n",
    "    if \"metrics\" in g and isinstance(g[\"metrics\"], dict) and \"synthetic\" in g[\"metrics\"]:\n",
    "        synthetic = _as_np(g[\"metrics\"][\"synthetic\"])\n",
    "    else:\n",
    "        G = g.get(\"generator_ema\") or g.get(\"G_ema\") or g.get(\"generator\") or g.get(\"G\")\n",
    "        if G is None: raise NameError(\"No synthetic data found and no generator in memory.\")\n",
    "        noise_dim = g.get(\"noise_dim\", 128)\n",
    "        device = g.get(\"device\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        N, T, A = real.shape\n",
    "        G = G.to(device).eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(N, noise_dim, device=device)\n",
    "            synthetic = G(z).cpu().numpy()\n",
    "\n",
    "assert real.ndim == synthetic.ndim == 3 and real.shape[2] == synthetic.shape[2], \"Shapes must be [N,T,A] with same A.\"\n",
    "\n",
    "asset = 0  \n",
    "\n",
    "r = real[:, :, asset].reshape(-1)\n",
    "s = synthetic[:, :, asset].reshape(-1)\n",
    "\n",
    "# 1) Lower-tail Q–Q (0–10%)\n",
    "qs = np.linspace(0.0, 0.10, 40)\n",
    "rq = np.quantile(r, qs)\n",
    "sq = np.quantile(s, qs)\n",
    "\n",
    "plt.figure(figsize=(5.2,5.2))\n",
    "plt.plot(rq, sq, lw=2, label=\"Synthetic\")\n",
    "plt.plot(rq, rq, \"--\", lw=2, label=\"Real (y=x)\")\n",
    "plt.grid(alpha=0.4, linestyle=\"--\")\n",
    "plt.title(f\"Lower-tail Q–Q (0–10%) — Asset {asset+1}\")\n",
    "plt.xlabel(\"Real quantiles\"); plt.ylabel(\"Synthetic quantiles\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2) ES(α) curve (1–10%)\n",
    "def _es(x, a):\n",
    "    q = np.quantile(x, a)\n",
    "    return x[x <= q].mean()\n",
    "\n",
    "alphas = np.linspace(0.01, 0.10, 30)\n",
    "es_real = np.array([_es(r, a) for a in alphas])\n",
    "es_syn  = np.array([_es(s, a) for a in alphas])\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(alphas*100, es_real, lw=2, label=\"Real\")\n",
    "plt.plot(alphas*100, es_syn,  lw=2, label=\"Synthetic\")\n",
    "plt.title(f\"ES(α) Curve — Asset {asset+1}\")\n",
    "plt.xlabel(\"Alpha (%)\"); plt.ylabel(\"ES(α)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4dcf8-5326-45e0-9fd6-d1e3b6aaa567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53d39f-6b50-464f-adb8-89bced1c9b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298e8cf-f731-4012-a1a0-9b35c96c230c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae84e3-1a5f-4789-bd2f-b9dd98e1fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "for i in range(len(train_data)):\n",
    "    asset_idx = i % 3 \n",
    "    one_hot = [0]*3\n",
    "    one_hot[asset_idx] = 1\n",
    "    labels.append(one_hot)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95c0fc-47c7-4ef8-bbef-bcf5b25f76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "\n",
    "# Config / Hyperparameter\n",
    "\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE    = 64\n",
    "EPOCHS        = 100\n",
    "NOISE_DIM     = 32\n",
    "ALPHA         = 0.05          \n",
    "LAMBDA_TAIL   = 10.0          \n",
    "PINBALL_W     = 0.5          \n",
    "WINDOW_W      = 12            \n",
    "USE_EMA       = True\n",
    "EMA_DECAY     = 0.999\n",
    "LR_D, LR_G    = 2e-4, 1e-4    \n",
    "\n",
    "\n",
    "\n",
    "N, T, A = train_data.shape\n",
    "assert T >= WINDOW_W, \"Window length WINDOW_W must be <= sequence length T.\"\n",
    "\n",
    "\n",
    "\n",
    "def build_condition(data, W):\n",
    "\n",
    "    cond_window = data[:, :W, :]                      \n",
    "    cond_vol    = cond_window.std(axis=1)             \n",
    "    cond = np.concatenate(\n",
    "        [cond_window.reshape(data.shape[0], -1), cond_vol],\n",
    "        axis=1\n",
    "    )                                                 \n",
    "    # Normalize condition features \n",
    "    mean = cond.mean(axis=0, keepdims=True)\n",
    "    std  = cond.std(axis=0, keepdims=True) + 1e-8\n",
    "    cond_norm = (cond - mean) / std\n",
    "    return cond_norm.astype(np.float32)\n",
    "\n",
    "train_cond = build_condition(train_data, WINDOW_W)     \n",
    "val_cond   = build_condition(val_data, WINDOW_W)\n",
    "\n",
    "COND_DIM = train_cond.shape[1]\n",
    "INPUT_DIM = T * A                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685fc42d-0eae-4469-ac32-936e86b6e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & DataLoader\n",
    "# -----------------------------\n",
    "class ConditionalCryptoDataset(Dataset):\n",
    "    def __init__(self, data_array, condition_array):\n",
    "        self.data = torch.tensor(data_array, dtype=torch.float32)\n",
    "        self.cond = torch.tensor(condition_array, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx], self.cond[idx]\n",
    "\n",
    "train_ds = ConditionalCryptoDataset(train_data, train_cond)\n",
    "val_ds   = ConditionalCryptoDataset(val_data,   val_cond)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f3a46-3e9e-4e19-b8cc-b3e86d904366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minibatch-StdDev trick (FC)\n",
    "# -----------------------------\n",
    "class MinibatchStdDev(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, x):  \n",
    "        B = x.size(0)\n",
    "        if B < 2:\n",
    "            return x\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        var  = (x - mean).pow(2).mean(dim=0, keepdim=True)\n",
    "        std  = torch.sqrt(var + self.eps)         \n",
    "        std_feat = std.mean().expand(B, 1)        \n",
    "        return torch.cat([x, std_feat], dim=1)   \n",
    "\n",
    "# -----------------------------\n",
    "# Conditional Generator / Discriminator\n",
    "# -----------------------------\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim, cond_dim, output_dim):\n",
    "        super().__init__()\n",
    "        in_dim = noise_dim + cond_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),    nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),    nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "        self.T = T\n",
    "        self.A = A\n",
    "    def forward(self, z, c):                  \n",
    "        x = torch.cat([z, c], dim=1)\n",
    "        out = self.net(x)                     \n",
    "        return out.view(z.size(0), self.T, self.A)  \n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim):\n",
    "        super().__init__()\n",
    "        in_dim = input_dim + cond_dim\n",
    "        self.feat = nn.Sequential(\n",
    "            spectral_norm(nn.Linear(in_dim, 128)), nn.LeakyReLU(0.2),\n",
    "            spectral_norm(nn.Linear(128, 64)),     nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.mbstd = MinibatchStdDev()\n",
    "        self.out   = spectral_norm(nn.Linear(64+1, 1))  \n",
    "    def forward(self, x, c):                 \n",
    "        x = x.view(x.size(0), -1)            \n",
    "        h = self.feat(torch.cat([x, c], dim=1))  \n",
    "        h = self.mbstd(h)                         \n",
    "        return self.out(h)                       \n",
    "\n",
    "G = ConditionalGenerator(NOISE_DIM, COND_DIM, INPUT_DIM).to(DEVICE)\n",
    "D = ConditionalDiscriminator(INPUT_DIM, COND_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cea50-750a-4a8a-9ce3-2e9d15d91f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expected_shortfall(x, alpha=ALPHA, dim=0):\n",
    "    \n",
    "    k = max(1, int(alpha * x.size(dim)))\n",
    "    vals, _ = torch.sort(x, dim=dim)\n",
    "    return vals.narrow(dim, 0, k).mean(dim=dim)\n",
    "\n",
    "def es_matching_loss(real, fake, alpha=ALPHA):\n",
    "  \n",
    "    B, Tt, Aa = real.shape\n",
    "    r = real.reshape(B*Tt, Aa)\n",
    "    f = fake.reshape(B*Tt, Aa)\n",
    "    es_r = expected_shortfall(r, alpha=alpha, dim=0)\n",
    "    es_f = expected_shortfall(f, alpha=alpha, dim=0) \n",
    "    return torch.mean((es_f - es_r).abs())\n",
    "\n",
    "def quantile_pinball_loss(y_pred, y_true, q=0.05):\n",
    "    e = y_true - y_pred\n",
    "    return torch.mean(torch.maximum(q*e, (q-1)*e))\n",
    "\n",
    "\n",
    "# Optimizers, loss, EMA\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=(0.5, 0.999))\n",
    "\n",
    "if USE_EMA:\n",
    "    G_ema = ConditionalGenerator(NOISE_DIM, COND_DIM, INPUT_DIM).to(DEVICE)\n",
    "    G_ema.load_state_dict(G.state_dict())\n",
    "    @torch.no_grad()\n",
    "    def ema_update(target, source, decay=EMA_DECAY):\n",
    "        for p_t, p_s in zip(target.parameters(), source.parameters()):\n",
    "            p_t.data.mul_(decay).add_(p_s.data, alpha=1-decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd3c1e-4c8e-4e89-8d10-ff8606ef973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    G.train(); D.train()\n",
    "    for real_batch, cond in train_loader:\n",
    "        real_batch = real_batch.to(DEVICE)   \n",
    "        cond       = cond.to(DEVICE)       \n",
    "        B = real_batch.size(0)\n",
    "\n",
    "        # ======== Train D ========\n",
    "        z = torch.randn(B, NOISE_DIM, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            fake_batch = G(z, cond)\n",
    "\n",
    "        # label smoothing\n",
    "        real_labels = torch.empty(B, 1, device=DEVICE).uniform_(0.9, 1.0)\n",
    "        fake_labels = torch.empty(B, 1, device=DEVICE).uniform_(0.0, 0.1)\n",
    "\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "        d_real = D(real_batch, cond)\n",
    "        d_fake = D(fake_batch, cond)\n",
    "        d_loss = criterion(d_real, real_labels) + criterion(d_fake, fake_labels)\n",
    "        d_loss.backward()\n",
    "        opt_D.step()\n",
    "\n",
    "        # ======== Train G (GAN + Tail) ========\n",
    "        z = torch.randn(B, NOISE_DIM, device=DEVICE)\n",
    "        fake_batch = G(z, cond)\n",
    "\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "        d_fake = D(fake_batch, cond)\n",
    "        gan_loss = criterion(d_fake, real_labels)  \n",
    "\n",
    "        es_loss = es_matching_loss(real_batch, fake_batch, alpha=ALPHA)\n",
    "\n",
    "        idx_t   = torch.randint(0, T, (B,), device=DEVICE)\n",
    "        r_samp  = real_batch[torch.arange(B), idx_t, :] \n",
    "        f_samp  = fake_batch[torch.arange(B), idx_t, :] \n",
    "        q05_loss = quantile_pinball_loss(f_samp, r_samp, q=0.05)\n",
    "\n",
    "        g_loss = gan_loss + LAMBDA_TAIL * es_loss + PINBALL_W * q05_loss\n",
    "        g_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(G.parameters(), max_norm=5.0)  \n",
    "        opt_G.step()\n",
    "\n",
    "        if USE_EMA:\n",
    "            ema_update(G_ema, G, EMA_DECAY)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | D: {d_loss.item():.4f} | G: {g_loss.item():.4f} | ES: {es_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70915e9-1784-41c0-b494-7bbd93e0d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_cgan_table(\n",
    "    generator,          \n",
    "    val_loader,        \n",
    "    noise_dim,          \n",
    "    device,             \n",
    "    alpha=0.05,\n",
    "    fisher_excess=True, \n",
    "    latent=\"gaussian\",  \n",
    "    df=3.0              \n",
    "):\n",
    "    # ---- gather real + cond ----\n",
    "    Xs, Cs = [], []\n",
    "    for x, c in val_loader:\n",
    "        Xs.append(x.cpu().numpy())\n",
    "        Cs.append(c.cpu().numpy())\n",
    "    real_np = np.concatenate(Xs, 0)  \n",
    "    cond_np = np.concatenate(Cs, 0) \n",
    "    N, T, A = real_np.shape\n",
    "\n",
    "    # ---- generate synthetic with same conditions ----\n",
    "    generator = generator.to(device).eval()\n",
    "    synth_chunks = []\n",
    "    bs = 512\n",
    "    for i in range(0, N, bs):\n",
    "        c = torch.tensor(cond_np[i:i+bs], dtype=torch.float32, device=device)\n",
    "        if latent.lower() == \"student_t\":\n",
    "            from torch.distributions import StudentT\n",
    "            z = StudentT(df=df).sample((c.size(0), noise_dim)).to(device)\n",
    "        else:\n",
    "            z = torch.randn(c.size(0), noise_dim, device=device)\n",
    "        y = generator(z, c).cpu().numpy()   \n",
    "        synth_chunks.append(y)\n",
    "    synth_np = np.concatenate(synth_chunks, 0)     \n",
    "\n",
    "    # ---- flatten over N*T for per-asset stats ----\n",
    "    real_flat  = real_np.reshape(-1, A)             \n",
    "    synth_flat = synth_np.reshape(-1, A)\n",
    "\n",
    "    # ---- print table  ----\n",
    "    print(f\"{'Metric':<12} | \" + \" | \".join([f\"Asset {i+1:>2}\" for i in range(A)]))\n",
    "    print(\"-\" * (14 + A*14))\n",
    "\n",
    "    # VaR\n",
    "    real_var  = [np.quantile(real_flat[:, i],  alpha) for i in range(A)]\n",
    "    synth_var = [np.quantile(synth_flat[:, i], alpha) for i in range(A)]\n",
    "    print(\"VaR (5%)    | \" + \" | \".join([f\"{v:10.5f}\" for v in real_var]))\n",
    "    print(\"            | \" + \" | \".join([f\"{v:10.5f}\" for v in synth_var]))\n",
    "\n",
    "    # ES\n",
    "    real_es  = [real_flat[:, i][real_flat[:, i]  <= real_var[i]].mean()  for i in range(A)]\n",
    "    synth_es = [synth_flat[:, i][synth_flat[:, i] <= synth_var[i]].mean() for i in range(A)]\n",
    "    print(\"ES (5%)     | \" + \" | \".join([f\"{e:10.5f}\" for e in real_es]))\n",
    "    print(\"            | \" + \" | \".join([f\"{e:10.5f}\" for e in synth_es]))\n",
    "\n",
    "    # Skewness\n",
    "    real_sk = [skew(real_flat[:, i],  bias=False) for i in range(A)]\n",
    "    syn_sk  = [skew(synth_flat[:, i], bias=False) for i in range(A)]\n",
    "    print(\"Skewness    | \" + \" | \".join([f\"{s:10.5f}\" for s in real_sk]))\n",
    "    print(\"            | \" + \" | \".join([f\"{s:10.5f}\" for s in syn_sk]))\n",
    "\n",
    "    # Kurtosis\n",
    "    real_ku = [kurtosis(real_flat[:, i],  bias=False, fisher=fisher_excess) for i in range(A)]\n",
    "    syn_ku  = [kurtosis(synth_flat[:, i], bias=False, fisher=fisher_excess) for i in range(A)]\n",
    "    print(\"Kurtosis    | \" + \" | \".join([f\"{k:10.5f}\" for k in real_ku]))\n",
    "    print(\"            | \" + \" | \".join([f\"{k:10.5f}\" for k in syn_ku]))\n",
    "\n",
    "    return {\n",
    "        \"synthetic\": synth_np,\n",
    "        \"VaR_real\": np.array(real_var),   \"VaR_synth\": np.array(synth_var),\n",
    "        \"ES_real\":  np.array(real_es),    \"ES_synth\":  np.array(synth_es),\n",
    "        \"Skew_real\": np.array(real_sk),   \"Skew_synth\": np.array(syn_sk),\n",
    "        \"Kurt_real\": np.array(real_ku),   \"Kurt_synth\": np.array(syn_ku),\n",
    "    }\n",
    "\n",
    "\n",
    "G_eval = G_ema if 'G_ema' in globals() and G_ema is not None else G\n",
    "\n",
    "metrics_cgan = evaluate_cgan_table(\n",
    "    generator=G_eval,\n",
    "    val_loader=val_loader,  \n",
    "    noise_dim=NOISE_DIM,\n",
    "    device=DEVICE,\n",
    "    alpha=0.05,\n",
    "    fisher_excess=True,\n",
    "    latent=\"gaussian\"        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec28f33e-2d5d-4b50-8a30-52f02b566671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ccbed-01a7-4294-99a7-330555a45e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cad4e-dfa6-4880-a91f-905db577328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\"figure.dpi\": 140})\n",
    "\n",
    "# --- helpers ---\n",
    "@torch.no_grad()\n",
    "def collect_real_and_cond(loader):\n",
    "    Xs, Cs = [], []\n",
    "    for batch in loader:\n",
    "        if isinstance(batch, (tuple, list)) and len(batch) == 2:\n",
    "            x, c = batch\n",
    "        else:\n",
    "            x, c = batch, torch.zeros(batch.size(0), 1)\n",
    "        Xs.append(x.cpu().numpy())\n",
    "        Cs.append(c.cpu().numpy())\n",
    "    return np.concatenate(Xs, 0), np.concatenate(Cs, 0)  \n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_cgan(G, cond_np, noise_dim, device):\n",
    "    G.eval()\n",
    "    out = []\n",
    "    bs = 512\n",
    "    for i in range(0, cond_np.shape[0], bs):\n",
    "        c = torch.tensor(cond_np[i:i+bs], dtype=torch.float32, device=device)\n",
    "        z = torch.randn(c.size(0), noise_dim, device=device)\n",
    "        y = G(z, c).cpu().numpy()  \n",
    "        out.append(y)\n",
    "    return np.concatenate(out, 0)\n",
    "\n",
    "def momentum_pnl_from_returns(rets):\n",
    "    # rets: [N, T, A] log-returns\n",
    "    r = rets[:, 1:, :]\n",
    "    signal = (rets[:, :-1, :] > 0).astype(int) * 2 - 1  \n",
    "    return (signal * r).sum(axis=1)  # [N, A]\n",
    "\n",
    "# --- 1) gather real & cond from validation ---\n",
    "real_np, cond_np = collect_real_and_cond(val_loader)   \n",
    "N, T, A = real_np.shape\n",
    "\n",
    "# --- 2) generate CGAN synthetic with same conditions ---\n",
    "G_eval = G_ema if \"G_ema\" in globals() and G_ema is not None else G \n",
    "synthetic_np = sample_cgan(G_eval, cond_np, NOISE_DIM, DEVICE)       \n",
    "\n",
    "# --- 3) compute PnL per sequence ---\n",
    "real_pnl  = momentum_pnl_from_returns(real_np)     \n",
    "synth_pnl = momentum_pnl_from_returns(synthetic_np)\n",
    "\n",
    "# --- 4) simple PnL hist overlays  ---\n",
    "for a in range(A):\n",
    "    rp, sp = real_pnl[:, a], synth_pnl[:, a]\n",
    "    mn, mx = float(min(rp.min(), sp.min())), float(max(rp.max(), sp.max()))\n",
    "    bins = np.linspace(mn, mx, 80)\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(rp, bins=bins, alpha=0.6, density=True, label=\"Real PnL\")\n",
    "    plt.hist(sp, bins=bins, alpha=0.6, density=True, label=\"CGAN PnL\")\n",
    "    plt.title(f\"PnL Distribution — Asset {a+1}\")\n",
    "    plt.xlabel(\"Strategy PnL\"); plt.ylabel(\"Density\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a753a40-f3d9-40b9-8b0f-fa01dd7b6137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99c29e-81af-4b61-825e-66cd22c1017d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68d77f-078e-425e-b4a7-da492689f812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9d2e0c-d77c-4f6f-a060-3489c71cca1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe56ab8-71c4-4a98-a2c3-75b326cae03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aeb2c2-1e02-46a4-9002-f0421273d9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521f44d-7d9d-4cfb-a164-bcad1d50178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001c3f7a-074e-424d-a75d-025d3ddea377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06cfa3-6424-4adc-9ca2-96b93f353554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
